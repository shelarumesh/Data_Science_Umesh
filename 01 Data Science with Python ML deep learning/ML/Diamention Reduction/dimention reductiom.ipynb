{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLdV/5KHYffGFKgABRHuUM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NyDBsGOsE7fZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.simplefilter(action='ignore')"]},{"cell_type":"markdown","source":["Data Scientist ---> Predictive Analytics (ML)\n","\n","--> Maximum Accuracy\n","\n","--> Cross Validation\n","\n","--> Maximum Data --> Good Accuracy\n","- upto 5% can be drop --> without any logic\n","- example 200 record * 5 columns = 1000 values   --5% is--> 50 records (5%)\n","-  if you have to drop >5% data, then you have prove statistically\n","- droping the unimportant input variable is called **Dimension Reduction**\n","\n","**Dimension Reduction** :\n","#1. Features Selection\n","\n","  \n","#A. Filter Method -->\n","  Before Modelling\n","  - drop record which are unique( like: ID,S.No,name, email, phone number)\n","  - drop features that has all records with same value (std. deviation =0)\n","  - drop the feature, that has > 30% of data missing\n","\n"," **For Regression Project** ðŸ‡°\n"," - drop the features which has low correlation  ---> Continuous Variable\n"," - drop the feature, that the has no of inpute variable relation with output variable base on annova test\n"," - when there is colinearity problem between two input variable--> drop any one\n"," - features base on VIF (i.e which has high VIF value)\n","\n"," **Classification** ðŸ‡°\n"," - drop the inpute feature, that has low information gain with output feature\n"," - drop the inpute feature, that has no relation with output variable base on Chisqare test\n","\n","#B. Wrapper Method -->\n","  linear regression only\n","feature Elimination technique\n","- drop the feature base on p value\\\n","      - forward p value --->\n","      - Backwored p value --->\n","#C. Embedded Method -->\n","  After modeling\n","  - Lasso - after applying lasso regression, it identify the unimportant variable by macking the coefficient as zero\n","  - Decision Tree -\n","  - Bagging (Random Forest) -\n","  - Boosting (adaboost, GradientBoost, XGBoost) -\n","\n","#2. Feature Extraction\n","_____\n","Principal Component Analysis\n","____\n","Fri - save the model using pickle, joblib & deployment using flask\n","\n","Mon - Capstone Project\n","\n","- Data Preprocessing\n","- RMSE\n","      - High Score\n","      - Low Score"],"metadata":{"id":"KpOZ0kEOFASD"}},{"cell_type":"markdown","source":["**Featurs Selection : **\n","1. Dimention Reductiom\n","2. we will identify directly the un-importtant features and drop them\n","\n","\n","\n","**PCA : Features Extraction **\n","1. Dimention Reduction\n","2. will transform into a new dataset with less no of features\n","\n","steps to calculate PCA\n","1. Standardization\n","2. covariance\n","3. Eigen value\n","|A-YI|=0\n","where Y is eigen value\n","4. Calculate the Principle componants\n","5. Reduced the un importance one"],"metadata":{"id":"aRYJaAjNP02F"}},{"cell_type":"markdown","source":["#Deployments :\n","\n"],"metadata":{"id":"hpQYlV9tenPn"}},{"cell_type":"code","source":[],"metadata":{"id":"ZcVhwgMpPt3s"},"execution_count":null,"outputs":[]}]}